{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Morphologies-Topologies_Stylegan2-ada_approxPoly_DSC_3574.ipynb","private_outputs":true,"provenance":[{"file_id":"https://github.com/ArthurFDLR/GANightSky/blob/main/GANightSky.ipynb","timestamp":1635805540230}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cPI5E5y0pujD"},"source":["\n","\n","# ðŸš€ StyleGan2-ADA for Google Colab\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SxEDjO7jG6dQ"},"source":["<video src=\"https://github.com/phylumcollective/bacteria/blob/main/Morphologies-Topologies/generative/gan/out/walk-z-noiseloop-seed0-24fps.mp4?raw=true\" width=\"512\" height=\"512\" muted autoplay loop />&nbsp;</video>\n"]},{"cell_type":"markdown","metadata":{"id":"jzZtdtcYQ38l"},"source":["based on [this notebook](https://colab.research.google.com/github/ArthurFDLR/GANightSky/blob/main/GANightSky.ipynb#scrollTo=Fxu7CA0Qb1Yd)"]},{"cell_type":"markdown","metadata":{"id":"ktqaMJUZuOl7"},"source":["1.   [Install StyleGAN2-ADA on your Google Drive](#scrollTo=5YcUMPQp6ipP)\n","2.   [Train a custom model](#scrollTo=Ti11YiPAiQpb)\n","3.   [Generate images from pre-trained model](#scrollTo=f0A9ZNtferpk)\n","4.   [Latent space exploration](#scrollTo=5yG1UyHXXqsO)\n"]},{"cell_type":"markdown","metadata":{"id":"5YcUMPQp6ipP"},"source":["## Install StyleGAN2-ADA on your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"SI_i1MwgpzOD"},"source":["StyleGAN2-ADA only works with Tensorflow 1. Run the next cell before anything else to make sure weâ€™re using TF1 and not TF2.\n"]},{"cell_type":"code","metadata":{"id":"iKYAU7Wub3WW"},"source":["%tensorflow_version 1.x\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzLk6hYzOy5Y"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZbwgNqRYPndP"},"source":["#<<----<strong>Transfer Files between different Cloud Drives using rclone\n","\n","#RESUME Supported :)\n","#Mega, OneDrive, Google Drive, Shared Drive, etc..\n","#You can use others too :)</strong>\n","\n","#Subscribe: <a href=\"https://www.youtube.com/boostupstation\">BoostUpStation</a>"]},{"cell_type":"code","metadata":{"id":"RzPulZDDQLg5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuvdTmE_Ersm"},"source":["#@title <<----<strong>Install rClone</strong>\n","%%capture\n","AUTO_RECONNECT = True #@param {type:\"boolean\"}\n","RCLONE = True #@param {type:\"boolean\"}\n","#@markdown Check AUTO_RECONNECT to prevent notebook from disconnecting!\n","\n","from os import makedirs\n","makedirs(\"/root/.config/rclone\", exist_ok = True) \n","  \n","if RCLONE==True:\n","  !curl https://rclone.org/install.sh | sudo bash\n","\n","if AUTO_RECONNECT:\n","  import IPython\n","  from google.colab import output\n","\n","  display(IPython.display.Javascript('''\n","  function ClickConnect(){\n","    btn = document.querySelector(\"colab-connect-button\")\n","    if (btn != null){\n","      console.log(\"Click colab-connect-button\"); \n","      btn.click() \n","      }\n","    \n","    btn = document.getElementById('ok')\n","    if (btn != null){\n","      console.log(\"Click reconnect\"); \n","      btn.click() \n","      }\n","    }\n","    \n","  setInterval(ClickConnect,60000)\n","  '''))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3LCtCOnQ6oq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-S7hYTzLpZK"},"source":["#@title <-----<strong>Upload your  Rclone.config file</strong>\n","def moveConfig():\n","  !mv rclone.conf /root/.config/rclone/rclone.conf\n","\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","moveConfig()\n","print(\"Moved rclone.conf to /root/.config/rclone/rclone.conf\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1cNs7RvWRFqa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4TZEfjtSF5C"},"source":["# <center>Rclone Mount/Unmount/Copy </center>\n","<center><img src=\"https://forum.rclone.org/uploads/default/original/2X/d/da6ccb2784ff3fa73d9339300530e0aae4d6cebd.png\" alt=\"rclone forum\" width=150></center>"]},{"cell_type":"code","metadata":{"id":"st8Q_ZN27ovN"},"source":["#@markdown <center><h3>Rclone MOUNT / UNMOUNT</h3>Mount the remote as file system on a mountpoint.</center>\n","import os\n","from IPython.display import HTML, clear_output\n","import uuid\n","import ipywidgets as widgets\n","from google.colab import output\n","import re\n","##########################################\n","\n","class MakeButton(object):\n","  def __init__(self, title, callback, style):\n","    self._title = title\n","    self._callback = callback\n","    self._style = style\n","  def _repr_html_(self):\n","    callback_id = 'button-' + str(uuid.uuid4())\n","    output.register_callback(callback_id, self._callback)\n","    if self._style != \"\":\n","      style_html = \"p-Widget jupyter-widgets jupyter-button widget-button mod-\" + self._style\n","    else:\n","      style_html = \"p-Widget jupyter-widgets jupyter-button widget-button\"\n","    template = \"\"\"<button class=\"{style_html}\" id=\"{callback_id}\">{title}</button>\n","        <script>\n","          document.querySelector(\"#{callback_id}\").onclick = (e) => {{\n","            google.colab.kernel.invokeFunction('{callback_id}', [], {{}})\n","            e.preventDefault();\n","          }};\n","        </script>\"\"\"\n","    html = template.format(title=self._title, callback_id=callback_id, style_html=style_html)\n","    return html\n","  \n","def ShowAC():\n","  clear_output(wait=True)\n","  display(\n","      widgets.HBox(\n","          [widgets.VBox(\n","              [widgets.HTML(\n","                  '''<h3 style=\"font-family:Trebuchet MS;color:#4f8bd6;margin-top:0px;\">\n","                  Rclone available config...</h3>\n","                  '''\n","                  ),\n","               mountNam]\n","               )\n","          ]\n","          )\n","      )\n","  \n","  display(HTML(\"<br>\"), MakeButton(\"Mount\", MountCMD, \"primary\"),\n","          MakeButton(\"Unmount\", unmountCMD, \"danger\"))\n","content = open(\"/root/.config/rclone/rclone.conf\").read()\n","avCon = re.findall(r\"^\\[(.+)\\]$\", content, re.M)\n","mountNam = widgets.Dropdown(options=avCon)\n","cache_path=\"/content/temp/rCloneTemp\"\n","def MountCMD():\n","    mPoint = f\"/content/drives/{mountNam.value}\"\n","    os.makedirs(mPoint, exist_ok=True)\n","    !rclone mount $mountNam.value: $mPoint --user-agent 'Mozilla' --buffer-size 256M --transfers 10 --vfs-cache-mode minimal --vfs-read-chunk-size 500M --vfs-cache-max-size 50G --vfs-cache-max-age 0h0m1s --vfs-cache-poll-interval 0m1s --cache-dir '/content/temp/rCloneTemp' --allow-other --daemon \n","\n","    if os.path.isdir(mPoint)== True:\n","      print(f\"Mount success! - \\t{mPoint}\")\n","    else:\n","      print(f\"Mount failed! - \\t{mPoint}\")\n","\n","def unmountCMD():\n","  mPoint = f\"/content/drives/{mountNam.value}\"\n","  if os.system(f\"fusermount -uz {mPoint}\") == 0:\n","    runSh(f\"rm -r {mPoint}\")\n","    print(f\"Unmounted success! - \\t{mPoint}\")\n","  else:\n","    runSh(f\"fusermount -uz {mPoint}\", output=True)\n","\n","ShowAC()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19_1uXab3gND"},"source":["Then, mount your Drive to the Colab notebook: "]},{"cell_type":"code","metadata":{"id":"pxxYlEKI9Gis"},"source":["# from google.colab import drive\n","from pathlib import Path\n","\n","#content_path = Path('/').absolute() / 'content'\n","#drive_path = content_path / 'drive'\n","#try:\n","  #drive.mount(str(drive_path), force_remount=True)\n","  #COLAB = True\n","  #print(\"Note: using Google CoLab\")\n","#except:\n","  #print(\"Note: not using Google CoLab\")\n","  #COLAB = False\n","\n","content_path = Path('/').absolute() / 'content'\n","drive_path = content_path / 'drives'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WMhv7Q1lTcE"},"source":["download phylum bacteria repo if necessary"]},{"cell_type":"code","metadata":{"id":"dlrY_nuHmFC3"},"source":["%cd /content/drives/mega/phylum"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPJLJ0VflSWj"},"source":["! git clone https://carloscastellanos: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@github.com/phylumcollective/bacteria.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epV6TDzAjox1"},"source":["Finally, run this cell to install StyleGAN2-ADA on your Drive. If youâ€™ve already installed the repository, it will skip the installation process and only check for updates. If you havenâ€™t installed it, it will install all the necessary files. Beside, **in**, **out**, **datasets** and **training** folders are generated for data storage. Everything will be available on your Google Drive in the folder **StyleGAN2-ADA** even after closing this Notebook."]},{"cell_type":"code","metadata":{"id":"8HX77jscX2zV"},"source":["stylegan2_repo_url  = 'https://github.com/carloscastellanos/stylegan2-ada' # 'https://github.com/dvschultz/stylegan2-ada' or https://github.com/NVlabs/stylegan2-ada\n","#project_path        = drive_path / 'MyDrive' / 'phylum' / 'bacteria' / 'Morphologies-Topologies' / 'generative' / 'gan'\n","project_path        = drive_path / 'mega' / 'phylum' / 'bacteria' / 'Morphologies-Topologies' / 'generative' / 'gan'\n","stylegan2_repo_path = project_path / 'models' / 'stylegan2-ada' / 'stylegan2-ada'\n","\n","# Create project folder if it does not exist\n","if not project_path.is_dir():\n","    %mkdir \"{project_path}\"\n","%cd \"{project_path}\"\n","\n","for dir in ['in', 'out', 'datasets', 'training']:\n","    if not (project_path / dir).is_dir():\n","        %mkdir {dir}\n","if not (project_path / 'datasets' / 'source').is_dir():\n","    %mkdir \"{project_path / 'datasets' / 'source'}\"\n","\n","# Download StyleGAN2-ada\n","!git config --global user.name \"carloscastellanos\"\n","!git config --global user.email \"carloscastellanossf@gmail.com\"\n","if stylegan2_repo_path.is_dir():\n","    !git -C \"{stylegan2_repo_path}\" fetch origin\n","    !git -C \"{stylegan2_repo_path}\" checkout origin/main -- *.py\n","else:\n","    print(\"Install StyleGAN2-ADA\")\n","    %cd \"{project_path / 'models' / 'stylegan2-ada'}\"\n","    !git clone {stylegan2_repo_url}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ti11YiPAiQpb"},"source":["## Train a custom model"]},{"cell_type":"markdown","metadata":{"id":"ioqYi9NzkUfG"},"source":["Once you have installed StyleGAN2-ADA on your Google Drive and set up the working directory, you can upload your training dataset images in the associated folder."]},{"cell_type":"code","metadata":{"id":"OlV5HIEqiZvu"},"source":["dataset_name = 'fraser_river' #'imagenet_0000'\n","datasets_source_path = project_path / 'datasets' / 'source' / (dataset_name + '.zip')\n","if datasets_source_path.is_dir():\n","    print(\"Dataset ready for import.\")\n","else:\n","    print('Upload your images dataset as {}'.format(datasets_source_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-y1-tvr5617d"},"source":["Unfortunately, large datasets might exceed the Google Drive quota after a few training batches. Indeed, StyleGAN2 download datasets multiple times during training. You might have to import your dataset in the local storage session. However, large files cannot be copy/paste from Drive *(Input/Output error)*. \n","\n","Run this cell to download your zipped dataset from your Drive and unzip it in the local session."]},{"cell_type":"code","metadata":{"id":"E5k5CNtzwXfS"},"source":["%cd ../"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQZGo4g5y7rh"},"source":["local_dataset_path = content_path / 'dataset'\n","if not local_dataset_path.is_dir():\n","    print(\"Importing dataset...\")\n","    %mkdir \"{local_dataset_path}\"\n","    %cp -a \"{project_path / 'datasets' / 'source' / (dataset_name + '.zip')}\" \"{local_dataset_path}\"\n","    print(\"Zip file succesfuly imported\")\n","else:\n","    print('Zip file allready imported')\n","\n","import zipfile\n","with zipfile.ZipFile(str(local_dataset_path / (dataset_name + '.zip')), 'r') as zip_ref:\n","    zip_ref.extractall(str(local_dataset_path))\n","print('Extraction completed')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlcQL2boXecv"},"source":["from os import listdir\n","from os.path import isfile, join\n","import os\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","\n","IMAGE_PATH = '/content/dataset/fraser_river' #'/content/dataset/imagenet_0000'\n","files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n","\n","base_size = None\n","for file in tqdm(files):\n","  file2 = os.path.join(IMAGE_PATH,file)\n","  img = Image.open(file2)\n","  sz = img.size\n","  if base_size and sz!=base_size:\n","    print(f\"Inconsistant size: {file2} {sz}\")\n","    %rm \"{file2}\"\n","  elif img.mode!='RGB':\n","    print(f\"Inconsistant color format: {file2}\")\n","  else:\n","    base_size = sz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeS9tDvt61VG"},"source":["### Convert dataset to .tfrecords"]},{"cell_type":"markdown","metadata":{"id":"_Q58MJbckLUc"},"source":["Next, we need to convert our image dataset to a format that StyleGAN2-ADA can read:`.tfrecords`.\n","\n","This can take a while."]},{"cell_type":"code","metadata":{"id":"IjH8kBDo3kFP"},"source":["\n","local_images_path = local_dataset_path / 'fraser_river' #'imagenet_0000'\n","local_dataset_path /= 'tfr'\n","\n","if (local_dataset_path).is_dir():\n","    print('\\N{Heavy Exclamation Mark Symbol} Dataset already created \\N{Heavy Exclamation Mark Symbol}')\n","    print('Delete current dataset folder ({}) to regenerate tfrecords.'.format(local_dataset_path))\n","else:\n","    %mkdir \"{local_dataset_path}\"\n","    !python \"{stylegan2_repo_path / 'dataset_tool.py'}\" create_from_images \\\n","        \"{local_dataset_path}\" \"{local_images_path}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8DvTupHzP2s_"},"source":["There are numerous arguments to tune the training of your model. To obtain nice results, you will certainly have to experiment. Here are the most popular parameters:\n","\n","\n","*   *mirror:* Should the images be mirrored vertically?\n","*   *mirrory:* Should the images be mirrored horizontally?\n","*   *snap:* How often should the model generate image samples and a network pickle (.pkl file)?\n","*   *resume:* Network pickle to resume training from?\n","\n","To see all the options, run the following ```help``` cell.\n","\n","Please note that Google Colab Pro gives access to V100 GPUs, which drastically decreases (~3x) processing time over P100 GPUs."]},{"cell_type":"code","metadata":{"id":"Fxu7CA0Qb1Yd"},"source":["!python \"{stylegan2_repo_path / 'train.py'}\" --help"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOftFoyiDU3s"},"source":["training_path = project_path / 'training' / dataset_name\n","if not training_path.is_dir():\n","    %mkdir \"{training_path}\"\n","\n","#how often should the model generate samples and a .pkl file\n","snapshot_count = 5\n","#should the images be mirrored left to right?\n","mirrored = True\n","#should the images be mirrored top to bottom?\n","mirroredY = False\n","#metrics? \n","metric_list = None\n","#augments\n","augs = 'bgc'\n","\n","# config default=auto, some others: \n","# stylegan2 (1024x1024/config F at 1024x1024)\n","# paper512 (BreCaHAD and AFHQ at 512x512)\n","# paper256 (FFHQ and LSUN Cat at 256x256)\n","# see stylegan2-ada GitHub repo for more\n","# base_cfg = 'auto'\n","\n","resume_from = 'ffhq512' # also ffhq256, ffhq1024, celebahq256\n","# resume_from = training_path / '00001-tfr-mirror-auto1-bgc-resumecustom' / 'network-snapshot-000100.pkl' # use this after first training run (and keep upadting if necessary)\n","\n","!python \"{stylegan2_repo_path / 'train.py'}\" --outdir=\"{training_path}\" \\\n","    --data=\"{local_dataset_path}\" --resume=\"{resume_from}\" \\\n","    --snap={snapshot_count} --augpipe={augs} \\\n","    --mirror={mirrored} --mirrory={mirroredY} \\\n","    --metrics={metric_list} #--dry-run"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6Uhzv1NNV1j"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0A9ZNtferpk"},"source":["## Generate images from pre-trained model\n","\n","You can finally generate images using a pre-trained network once everything is set-up. You can naturally use [your own model once it is trained](#scrollTo=Ti11YiPAiQpb&uniqifier=1) or use the ones NVLab published on [their website](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/).\n","\n","<p align=\"center\">\n","    <img\n","    alt=\"Night Sky Latent Walk\"\n","    width=\"450\" height=\"300\"\n","    src=\"https://github.com/ArthurFDLR/GANightSky/blob/main/.github/Random_Generation.png?raw=true\">\n","</p>"]},{"cell_type":"code","metadata":{"id":"xnlrb9QzgaGp"},"source":["%pip install opensimplex\n","!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images --help "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQqYjeRsfYD2"},"source":["from numpy import random\n","seed_init = random.randint(10000)\n","nbr_images = 6\n","\n","generation_from = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl'\n","\n","!python \"{stylegan2_repo_path / 'generate.py'}\" generate-images \\\n","    --outdir=\"{project_path / 'out'}\" --trunc=0.7 \\\n","    --seeds={seed_init}-{seed_init+nbr_images-1} --create-grid \\\n","    --network={generation_from}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5yG1UyHXXqsO"},"source":["## Latent space exploration"]},{"cell_type":"markdown","metadata":{"id":"kYLhEFGMtJIw"},"source":["It is also possible to explore the latent space associated with our model and [generate videos like this one](https://youtu.be/dcb4Ckpkx2o).\n"]},{"cell_type":"code","metadata":{"id":"veceGR6QYA93"},"source":["%pip install opensimplex\n","!python \"{stylegan2_repo_path / 'generate.py'}\" generate-latent-walk --help "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjsN05ksZYZ7"},"source":["from numpy import random\n","walk_types = ['line', 'sphere', 'noiseloop', 'circularloop']\n","latent_walk_path = project_path / 'out' / 'latent_walk'\n","if not latent_walk_path.is_dir():\n","    %mkdir \"{latent_walk_path}\"\n","\n","#explored_network = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl'\n","explored_network = project_path / 'training' / 'fraser_river' /'00003-tfr-mirror-auto1-bgc-resumecustom' / 'network-snapshot-000100.pkl'\n","\n","seeds = [random.randint(10000) for i in range(10)]\n","print(','.join(map(str, seeds)))\n","print(\"Base seeds:\", seeds)\n","!python \"{stylegan2_repo_path / 'generate.py'}\" generate-latent-walk --network=\"{explored_network}\" \\\n","    --outdir=\"{latent_walk_path}\" --trunc=0.7 --walk-type=\"{walk_types[2]}\" \\\n","    --seeds={','.join(map(str, seeds))} --frames {len(seeds)*20}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYXLW2cJKGbG"},"source":["## Growth agencies, path attributes, Adhesion algorithms \n"]},{"cell_type":"code","metadata":{"id":"e8C2CtbsHg2i"},"source":["import cv2\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def loadImg(s, read_as_float32=False, gray=False):\n","  if read_as_float32:\n","    img = cv2.imread(s).astype(np.float32) / 255\n","  else:\n","    img = cv2.imread(s)\n","  if gray:\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","  return img\n","\n","def scaleImg(img, scaleFactor=0.5):\n","  width = int(img.shape[1] * scaleFactor)\n","  height = int(img.shape[0] * scaleFactor)\n","  return cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrAeGdpsKD9V"},"source":["bacteria_path = drive_path / 'mega' / 'phylum' / 'bacteria' / 'Morphologies-Topologies' / 'image-processing' / 'img' / 'blossom' / 'DSC_3574.JPG'\n","print(bacteria_path)\n","\n","threshold = 115 # The cutoff for the threshold algorithm (0-255)\n","\n","# load image, convert to gray and scale down\n","img = loadImg(str(bacteria_path), gray=True)\n","img = scaleImg(img)\n","img2 = loadImg(str(bacteria_path))\n","img2 = scaleImg(img2)\n","\n","plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMNL2Wm5PzX6"},"source":["import numpy as np\n","\n","# create the random seeds based upon image dimensions\n","img_seeds = np.arange(1, (img.shape[0]*img.shape[1]) + 1).reshape(img.shape)\n","\n","# blur & threshold\n","imgBlur = cv2.medianBlur(img, 15)\n","ret, thresh = cv2.threshold(imgBlur, int(threshold), 255, cv2.THRESH_BINARY)\n","\n","# find Contours\n","contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n","\n","out = np.zeros_like(thresh)\n","\n","approxs = []\n","gen_seeds = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CAOCsY9b6BD"},"source":["# draw the contours (-2 removes plate edges)\n","for i in range(len(contours)-2):\n","  # Calculate area and remove small elements\n","  area = cv2.contourArea(contours[i])\n","  # -1 in 4th column means it's an external contour\n","  if hierarchy[0][i][3] == -1 and area > 566:\n","    M = cv2.moments(contours[i])\n","    # calculate x,y coordinate of centroid & draw it (also add the coords to the centerPoints array)\n","    cX = int(M[\"m10\"] / M[\"m00\"])\n","    cY = int(M[\"m01\"] / M[\"m00\"])\n","    if cX < img.shape[1] - 250:\n","      # contour approximation (\"smoothing\")\n","      epsilon = 0.01*cv2.arcLength(contours[i], True)\n","      approx = cv2.approxPolyDP(contours[i], epsilon, True)\n","      approxs.append(approx)\n","      # print(approx)\n","      cv2.drawContours(out, [approx], -1, (204, 204, 204), 3)\n","      cv2.drawContours(img2, [approx], -1, (204, 204, 204), 3)\n","      # cv2.drawContours(out, contours, i, (204, 204, 204), 3)\n","      # print(contours[i][0][0])\n","      # for a in approx:\n","        # for aa in a:\n","          # print(aa)\n","\n","for a in approxs:\n","  for cnt in a:\n","    coord = cnt[0]\n","    # remember numpy arrays are row/col while opencv are col/row (as is common for images)\n","    print(img_seeds[coord[1]][coord[0]])\n","    gen_seeds.append(img_seeds[coord[1]][coord[0]])\n","    print(coord)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AibOsH2acp4n"},"source":["plt.imshow(cv2.cvtColor(out, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBHCaOtCZ1U0"},"source":["plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSGbcYk6mGGr"},"source":["latent_walk_path = project_path / 'out' / 'latent_walk'\n","if not latent_walk_path.is_dir():\n","    %mkdir \"{latent_walk_path}\"\n","\n","#explored_network = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl'\n","explored_network = project_path / 'training' / 'fraser_river' /'00003-tfr-mirror-auto1-bgc-resumecustom' / 'network-snapshot-000100.pkl'\n","\n","#seeds = [random.randint(10000) for i in range(10)]\n","#print(','.join(map(str, seeds)))\n","#print(\"Base seeds:\", seeds)\n","!python \"{stylegan2_repo_path / 'generate.py'}\" generate-latent-walk --network=\"{explored_network}\" \\\n","    --outdir=\"{latent_walk_path}\" --trunc=0.7 --seeds={','.join(map(str, gen_seeds))} --frames {len(gen_seeds)*10}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9878RlntrDp"},"source":["## While you wait ...\n","\n","... learn more about Generative Adversarial Networks and StyleGAN2-ADA:\n","\n","*   [This Night Sky Does Not Exist](https://arthurfindelair.com/thisnightskydoesnotexist/): Generation of images from a model created using this Notebook on Google Colab Pro.\n","*   [This **X** Does Not Exist](https://thisxdoesnotexist.com/): Collection of sites showing the power of GANs.\n","*   [Karras, Tero, et al. _Analyzing and Improving the Image Quality of StyleGAN._ CVPR 2020.](https://arxiv.org/pdf/2006.06676.pdf): Paper published for the release of StyleGAN2-ADA.\n","*   [Official implementation of StyleGAN2-ADA](https://github.com/NVlabs/stylegan2-ada)\n","*   [StyleGAN v2: notes on training and latent space exploration](https://towardsdatascience.com/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3): Interesting article from Toward Data Science"]}]}